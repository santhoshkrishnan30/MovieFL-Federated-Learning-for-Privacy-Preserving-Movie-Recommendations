# MovieFL: Federated Learning for Privacy-Preserving Movie Recommendations

## Overview

**MovieFL** is a federated learning (FL) project that builds a privacy-preserving movie recommendation system using the MovieLens 100k dataset. The system trains a collaborative filtering model in a federated setup, where clients (representing users/devices) train locally and share model updates with a central server without exchanging raw data. This approach ensures user privacy while enabling collaborative model training. Key features include:

- **Federated Learning Setup**: A Flask-based server coordinates training across clients, aggregating model updates securely.
- **Differential Privacy**: Local model updates are perturbed with noise to enhance privacy.
- **Model Architecture**: A neural collaborative filtering model using embeddings for users and movies.
- **Evaluation**: Comparison of federated vs. centralized training, with plans for privacy analysis via model inversion attacks.

This project demonstrates how federated learning can balance model performance and user privacy in recommendation systems.

## Project Structure

- `server.py`: The central server that orchestrates federated learning, aggregates models, and evaluates the global model.
- `client.py`: The client script that performs local training and submits model updates to the server.
- `centralized_training.py`: Script for baseline centralized training (to be implemented).
- `model_inversion_attack.py`: Script for privacy analysis via model inversion attacks (to be implemented).
- `templates/index.html`: Dashboard for visualizing training metrics.
- `static/`: Directory for storing comparison plots generated by the server.

## Prerequisites

- **Python**: 3.9+
- **Libraries**:
  - `torch`
  - `pandas`
  - `numpy`
  - `scikit-learn`
  - `flask`
  - `flask-cors`
  - `requests`
  - `matplotlib`
  
Install dependencies using:
```bash
pip install torch pandas numpy scikit-learn flask flask-cors requests matplotlib
```

- **Dataset**: The project uses the MovieLens 100k dataset, automatically downloaded from `https://files.grouplens.org/datasets/movielens/ml-100k/u.data`.

## Setup and Usage

### 1. Clone the Repository
```bash
git clone https://github.com/<your-username>/moviefl.git
cd moviefl
```

### 2. Start the Server
Run the server to initialize the federated learning orchestration:
```bash
python server.py
```
- The server will start on `http://localhost:5000`.
- It initializes the global model and waits for client connections.

### 3. Run a Client
Run a client to participate in federated learning:
```bash
python client.py --client-id client1 --server-ip <server-ip> --server-port 5000
```
- Replace `<server-ip>` with the server’s IP (e.g., `localhost` for local testing or a remote IP like `qkr068n3-5000.inc1.devtunnels.ms`).
- The client will register, train locally, and submit model updates to the server.

### 4. Monitor Training
- The client logs training progress for each round, showing the loss per epoch.
- The server logs aggregation details, global model evaluation (RMSE and accuracy), and saves the final model.
- Access the dashboard at `http://localhost:5000` to visualize metrics (after implementing `index.html`).

### 5. Run Centralized Training (Optional)
To compare federated learning with a centralized baseline:
```bash
python centralized_training.py
```
- This script (to be implemented) will train the model centrally and save metrics to `centralized_metrics.pt`.

### 6. Perform Privacy Analysis (Optional)
To assess privacy risks:
```bash
python model_inversion_attack.py
```
- This script (to be implemented) will perform a model inversion attack and generate a privacy report.

## Results

### Training Progress
- **Federated Learning**:
  - Rounds: 5
  - Initial Loss (Round 1, Epoch 1): 2.2037
  - Final Loss (Round 5, Epoch 5): 0.6088
  - Loss Progression Across Rounds:
    - Round 1: 2.2037 → 1.1295
    - Round 2: 1.0687 → 0.8704
    - Round 3: 0.8418 → 0.7300
    - Round 4: 0.7165 → 0.6529
    - Round 5: 0.6485 → 0.6088
- **Centralized Baseline**: (To be implemented; placeholder metrics used for comparison plots)
  - Loss: 0.95 → 0.68
  - Accuracy: 0.80 → 0.89

### Visualizations
- Comparison plots (federated vs. centralized) are saved in `static/comparison_results.png` after training completes.
- Access the dashboard at `http://localhost:5000` to view live metrics (requires `index.html` implementation).

### Model Artifacts
- `global_model_round_<round>.pt`: Global model weights saved after each round.
- `final_federated_model.pt`: Final global model weights after training completes.

## Challenges and Solutions

- **Embedding Size Mismatch**: Initial runs failed due to inconsistent `n_users` and `n_movies` between server and client. Fixed by passing these values from the server to clients during registration.
- **Threading Issues**: The server’s orchestration thread stalled due to timing issues. Added a `threading.Event` to signal model submissions and ensure timely aggregation.
- **Client Termination Bug**: A typo (`client1` instead of `client_id`) caused the client to fail after training. Fixed by correcting the variable name.

## Next Steps

- **Implement Centralized Training**:
  - Complete `centralized_training.py` to generate baseline metrics for comparison.
- **Privacy Analysis**:
  - Implement `model_inversion_attack.py` to evaluate privacy risks and generate a report on data reconstruction risks.
- **Enhance Dashboard**:
  - Develop `templates/index.html` to create an interactive dashboard for visualizing training metrics in real-time.
- **Multi-Client Support**:
  - Test with multiple clients to simulate a real-world federated learning scenario.
- **Optimize Performance**:
  - Explore advanced aggregation methods (e.g., weighted aggregation) and tune hyperparameters for better convergence.

## Contributing

Contributions are welcome! Please open an issue or submit a pull request for bug fixes, feature additions, or improvements.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **Dataset**: MovieLens 100k dataset provided by GroupLens.
- **Libraries**: Built with PyTorch, Flask, and other open-source tools.
- **Guidance**: Thanks to the open-source community for federated learning resources and tutorials.